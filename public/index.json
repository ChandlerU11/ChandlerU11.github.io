[{"content":"\r\u003c!DOCTYPE html\u003e Class Imbalance and Problem Statement Class imbalance is a common problem when building classifiers in the machine learning world, and our awesome previously-scraped croc reviews data is unfortunately not so awesome from a class balance standpoint. Soon, we\u0026rsquo;ll assign binary class labels based on the rating a customer gave with their review where we\u0026rsquo;ll consider ratings of 2 stars (out of 5) or less to be negative sentiment and the remaining reviews as positive sentiment. As you\u0026rsquo;ll see in a moment, the vast majority of reviews belong to the positive sentiment class, and I think that\u0026rsquo;s great!\nHowever, I don\u0026rsquo;t believe Crocs reached the top of the shoe game by mistake. I\u0026rsquo;d be willing to bet the creators behind Crocs are more than willing to confront their flaws and improve upon them. Let\u0026rsquo;s pretend the good people behind Crocs have asked us to build an ML model to effectively classify the rare negative review for their product despite the severe class imbalance. They don\u0026rsquo;t mind a few misclassifications of the positive reviews here and there but would prefer there aren\u0026rsquo;t a ton of these instances.\nQuick Note on Positive / Negative Lingo Traditionally, the positive class in a binary labeled dataset is the minority class of most interest / importance, and that will hold true in this project. Apologies for any confusion, but going forward when I reference the positive class, I will be referencing the set of negative sentiment reviews.\nPositive Class (1) - Negative Sentiment Review :( Negative Class (0) - Positive Sentiment Review :) Below is a look at the class imbalance of our data.\nimport pandas as pd from collections import Counter import seaborn as sns df = pd.read_csv(\u0026#39;/content/drive/MyDrive/croc_reviews.csv\u0026#39;) df[\u0026#39;label\u0026#39;] = [0 if each \u0026gt; 2 else 1 for each in df[\u0026#39;rating\u0026#39;]] sns.countplot(x = df[\u0026#39;label\u0026#39;], hue = df[\u0026#39;label\u0026#39;]) print(\u0026#34;Only\u0026#34;, \u0026#34;{:.0%}\u0026#34;.format(label_count[1] / (label_count[1] + label_count[0])), \u0026#34;of our data belongs to the positive class.\u0026#34;) Only 6% of our data belongs to the positive class. Data Clean Up Before we start addressing the class imbalance issue, let\u0026rsquo;s clean up the reviews using the text cleaning function we\u0026rsquo;ve used before.\nimport re import string import nltk from nltk.corpus import stopwords from nltk.stem import PorterStemmer def clean_text(text): # Remove punctuation text = re.sub(f\u0026#34;[{re.escape(string.punctuation)}]\u0026#34;, \u0026#34;\u0026#34;, text) # Remove numbers text = re.sub(r\u0026#39;\\d+\u0026#39;, \u0026#39;\u0026#39;, text) # Convert to lowercase text = text.lower() # Remove stopwords nltk.download(\u0026#39;stopwords\u0026#39;, quiet=True) stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) tokens = nltk.word_tokenize(text) tokens = [word for word in tokens if word.lower() not in stop_words] # Stemming stemmer = PorterStemmer() tokens = [stemmer.stem(word) for word in tokens] # Join the tokens back into a single string cleaned_text = \u0026#39; \u0026#39;.join(tokens) return cleaned_text df[\u0026#39;clean_review\u0026#39;] = [clean_text(text_to_clean) for text_to_clean in df[\u0026#39;review\u0026#39;]] df review date rating label clean_review 0 !!!!!! E X C E L L E N T!!!!!!!!!! April 7, 2022 5.0 0 e x c e l l e n 1 \"They're crocs; people know what crocs are.\" April 3, 2021 5.0 0 theyr croc peopl know croc 2 - Quick delivery and the product arrived when ... March 19, 2023 5.0 0 quick deliveri product arriv compani said woul... 3 ...amazing \"new\" color!! who knew?? love - lov... July 17, 2022 5.0 0 amaz new color knew love love love 4 0 complaints from me; this is the 8th pair of ... June 4, 2021 5.0 0 complaint th pair croc ive bought like two mon... ... ... ... ... ... ... 9233 I will definitely be buying again in many colo... August 25, 2021 4.0 0 definit buy mani color reason materi feel thin... 9234 I wish I would have bought crocs a long time ago. April 8, 2021 5.0 0 wish would bought croc long time ago 9235 wonderful. Gorgeous blue; prettier in person! April 27, 2022 5.0 0 wonder gorgeou blue prettier person 9236 Wonerful. Very comfy, and there are no blister... April 8, 2021 5.0 0 woner comfi blister feet unlik brand one 9237 Work from home - high arch need good support a... May 22, 2023 5.0 0 work home high arch need good support comfort ... 9238 rows Ã— 5 columns\nStrategy: Performance Metrics and Dealing with Imbalanced Data Because we\u0026rsquo;re dealing with imbalanced data and are most concerned with identifying the minority / positive class, we will focus on improving the recall score of our models on the test set. We will also watch F2 score, a modified version of F1 score that increases the importance of recall in its calculation. Why F2 score? We are concerned with maximizing recall, but a model that predicts the minority class 100% of the time would achieve a perfect recall score. That doesn\u0026rsquo;t help us very much, and F2 score will give us an understanding of how well the model can differentiate between the two classes along with how well the model can identify positive samples. Below are the formulas to calculate the performance metrics.\nRecall - True Positive / (True Positive + False Negative) Precision - True Positive / (True Positive + False Positive) F2 Score - (5 * Precision * Recall) / (4 * Precision + Recall) There are a lot of ways to address the imbalanced data problem when training a classifier. In this project we\u0026rsquo;re going to adopt the following strategy:\nImplement multiple methods for resampling the data Train multiple baseline models using each of the resampling methods to see which resampling / model combo performs the best out of the box based on F2 and recall score Use GridsearchCV to tune the best baseline model for recall score whilst continuing to use the best resampling technique Alter the model\u0026rsquo;s decision threshold to maximize recall score Step 1: Implement Multiple Resampling Methods Thank goodness for the Imbalanced-Learn library because it makes data resampling much easier. The term resampling refers to the practice of balancing data by either selecting a subset of the majority class equal in size to the minority class (known as under-sampling) or artificially making the minority class as large as the majority (known as over-sampling). There are many different techniques for doing these processes, but we\u0026rsquo;ll try out the following:\nRandom Over-sampling - Randomly selects samples from the minority class and replaces them in the dataset. Random Under-sampling - Randomly selects and removes samples from the majority class. SMOTE Over-sampling - Synthetic Minority Over-sampling Technique (SMOTE) generates new samples for the minority class by imitating its features. The original paper explaining the technique can be found here. Cluster Under-sampling - Under-samples the majority class by replacing a cluster of majority samples by the cluster centroid of a KMeans algorithm. Keeps N majority samples by fitting the KMeans algorithm with N clusters to the majority class and keeping the centroids. The original paper explaining the technique can be found here from sklearn.cluster import MiniBatchKMeans from sklearn.metrics import fbeta_score, recall_score, accuracy_score, confusion_matrix, precision_recall_curve from imblearn.over_sampling import RandomOverSampler, SMOTE from imblearn.under_sampling import RandomUnderSampler, ClusterCentroids import numpy as np def rand_oversample(X, y): ros = RandomOverSampler(random_state=42) X_res, y_res = ros.fit_resample(X, y) return {\u0026#39;X_train\u0026#39;: X_res, \u0026#39;y_train\u0026#39;:y_res} def rand_undersample(X, y): rus = RandomUnderSampler(random_state=42) X_res, y_res = rus.fit_resample(X, y) return {\u0026#39;X_train\u0026#39;: X_res, \u0026#39;y_train\u0026#39;:y_res} def smote_oversample(X,y): sm = SMOTE(random_state=42) X_res, y_res = sm.fit_resample(X, y) return {\u0026#39;X_train\u0026#39;: X_res, \u0026#39;y_train\u0026#39;:y_res} def cluster_undersample(X, y): cc = ClusterCentroids( estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=42) X_res, y_res = cc.fit_resample(X, y) return {\u0026#39;X_train\u0026#39;: X_res, \u0026#39;y_train\u0026#39;:y_res} Holdout test set Let\u0026rsquo;s split our data into a train and test set. We will not alter the balance of the test set because we won\u0026rsquo;t be able to do that in the real world. However, our train set will undergo resampling as we find the best baseline model and resampling combo.\nfrom sklearn.feature_extraction.text import TfidfVectorizer from sklearn.model_selection import train_test_split def show_imb(label): imb = Counter(label) return str(imb[0]) + \u0026#39; Negative Samples | \u0026#39; + str(imb[1]) + \u0026#39; Positive Samples\u0026#39; # We\u0026#39;ll be using TF-IDF for this project vectorizer = TfidfVectorizer() vect_df = pd.DataFrame(vectorizer.fit_transform(df[\u0026#39;clean_review\u0026#39;]).todense(), columns=vectorizer.get_feature_names_out()) vect_df[\u0026#39;label\u0026#39;] = df[\u0026#39;label\u0026#39;].copy() # Train / test split df_train, df_test = train_test_split(vect_df, test_size = .1, random_state = 0) y_train, y_test = df_train[\u0026#39;label\u0026#39;].tolist(), df_test[\u0026#39;label\u0026#39;].tolist() X_train, X_test = df_train.drop([\u0026#39;label\u0026#39;], axis = 1), df_test.drop([\u0026#39;label\u0026#39;], axis = 1) print(\u0026#39;Holdout Test Set Class Balance: \u0026#39; + show_imb(y_test)) Holdout Test Set Class Balance: 872 Negative Samples | 52 Positive Samples Step 2: Train Multiple Baseline Classifiers Using Resampling Methods Let\u0026rsquo;s alter our training data using each of the resampling methods and train using each one. We\u0026rsquo;ll also train using the unsampled original training data as a baseline.\nsampled_data_dict = {\u0026#39;no_sample\u0026#39;: {\u0026#39;X_train\u0026#39;: X_train, \u0026#39;y_train\u0026#39;:y_train}, \u0026#39;random_oversample\u0026#39;: rand_oversample(X_train, y_train), \u0026#39;random_undersample\u0026#39;: rand_undersample(X_train, y_train), \u0026#39;smote_oversample\u0026#39;: smote_oversample(X_train, y_train), \u0026#39;cluster_undersample\u0026#39;: cluster_undersample(X_train, y_train)} # A look at the class balance following resampling for s in sampled_data_dict.keys(): txt = \u0026#39;Class Balance with {:\u0026lt;25}\u0026#39; + show_imb(sampled_data_dict[s][\u0026#39;y_train\u0026#39;]) print(txt.format(s + \u0026#39;:\u0026#39;)) Class Balance with no_sample: 7767 Negative Samples | 547 Positive Samples Class Balance with random_oversample: 7767 Negative Samples | 7767 Positive Samples Class Balance with random_undersample: 547 Negative Samples | 547 Positive Samples Class Balance with smote_oversample: 7767 Negative Samples | 7767 Positive Samples Class Balance with cluster_undersample: 547 Negative Samples | 547 Positive Samples As you can see, all the resampled training sets are now balanced except for the basline with no resampling. Let\u0026rsquo;s train some baseline classifiers with each resampling method and view the results on the unaltered test set.\nfrom tqdm.notebook import tqdm from sklearn.ensemble import RandomForestClassifier from sklearn.naive_bayes import GaussianNB from sklearn.linear_model import LogisticRegression # Baseline Models models_dict = {\u0026#39;Naive Bayes\u0026#39;: GaussianNB(), \u0026#39;Random Forest\u0026#39;: RandomForestClassifier(max_depth=10, random_state=0), \u0026#39;Logistic Regression\u0026#39;: LogisticRegression(random_state=0)} # Dataframe to view test results results_df = pd.DataFrame(columns = [\u0026#39;model_name\u0026#39;, \u0026#39;sampling_type\u0026#39;, \u0026#39;model\u0026#39;, \u0026#39;fbeta\u0026#39;, \u0026#39;recall\u0026#39;]) # Iterate over all models and all resampling techniques for m in tqdm(models_dict.keys()): for d in tqdm(sampled_data_dict.keys()): # Fit model on resampled data model = models_dict[m].fit(sampled_data_dict[d][\u0026#39;X_train\u0026#39;], sampled_data_dict[d][\u0026#39;y_train\u0026#39;]) # Predict on holdout test set preds = model.predict(X_test) # Add test results to dataframe results = [m, d, model, fbeta_score(y_test, preds, beta = 2), recall_score(y_test, preds)] results_df.loc[-1] = results results_df.index = results_df.index + 1 results_df = results_df.sort_index() results_df.sort_values(by = \u0026#39;recall\u0026#39;, ascending = False) model_name sampling_type model fbeta recall 5 Random Forest cluster_undersample (DecisionTreeClassifier(max_depth=10, max_feat... 0.442708 0.980769 0 Logistic Regression cluster_undersample LogisticRegression(random_state=0) 0.634518 0.961538 2 Logistic Regression random_undersample LogisticRegression(random_state=0) 0.664820 0.923077 7 Random Forest random_undersample (DecisionTreeClassifier(max_depth=10, max_feat... 0.574413 0.846154 3 Logistic Regression random_oversample LogisticRegression(random_state=0) 0.684713 0.826923 8 Random Forest random_oversample (DecisionTreeClassifier(max_depth=10, max_feat... 0.598291 0.807692 1 Logistic Regression smote_oversample LogisticRegression(random_state=0) 0.665584 0.788462 10 Naive Bayes cluster_undersample GaussianNB() 0.288462 0.750000 11 Naive Bayes smote_oversample GaussianNB() 0.286344 0.750000 13 Naive Bayes random_oversample GaussianNB() 0.283843 0.750000 14 Naive Bayes no_sample GaussianNB() 0.283843 0.750000 6 Random Forest smote_oversample (DecisionTreeClassifier(max_depth=10, max_feat... 0.523649 0.596154 12 Naive Bayes random_undersample GaussianNB() 0.341463 0.538462 4 Logistic Regression no_sample LogisticRegression(random_state=0) 0.115207 0.096154 9 Random Forest no_sample (DecisionTreeClassifier(max_depth=10, max_feat... 0.000000 0.000000 In terms of recall the random forest / cluster undersampling combo is the best, but it is quite weak in terms of F2 score, which means it will likely give us a lot of false positives in its predictions. The logistic regression / cluster undersample performs similarly in terms of recall, but maintains a much better F2 score, so we\u0026rsquo;ll move forward with that model! Let\u0026rsquo;s have a better look at its performance out of the box using a confusion matrix.\ndef show_confusion(y_test, pred): cf = confusion_matrix(y_test, pred) group_names = [\u0026#34;True Negatives\u0026#34;,\u0026#34;False Positives\u0026#34;,\u0026#34;False Negatives\u0026#34;,\u0026#34;True Positives\u0026#34;] group_perc = [cf.flatten()[0] / (cf.flatten()[0] + cf.flatten()[1]), cf.flatten()[1] / (cf.flatten()[0] + cf.flatten()[1]) , cf.flatten()[2] / (cf.flatten()[2] + cf.flatten()[3]), cf.flatten()[3] / (cf.flatten()[2] + cf.flatten()[3])] group_perc_str = [\u0026#34;{:.0%}\u0026#34;.format(each) for each in group_perc] group_counts = [\u0026#34;{0:0.0f}\u0026#34;.format(value) for value in cf.flatten()] labels = labels = [f\u0026#34;{v1} {v2}\\n{v3} of class total\u0026#34; for v1, v2, v3 in zip(group_counts, group_names, group_perc_str)] labels = np.asarray(labels).reshape(2,2) group_perc = np.asarray(group_perc).reshape(2,2) print(sns.heatmap(group_perc, annot=labels, fmt = \u0026#39;\u0026#39;, cmap=\u0026#39;Greens\u0026#39;)) best_baseline_model = results_df.loc[(results_df[\u0026#39;model_name\u0026#39;] == \u0026#39;Logistic Regression\u0026#39;) \u0026amp; (results_df[\u0026#39;sampling_type\u0026#39;] == \u0026#39;cluster_undersample\u0026#39;)][\u0026#39;model\u0026#39;][0] show_confusion(y_test, best_baseline_model.predict(X_test)) We\u0026rsquo;re already in a really good spot as we are correctly identifying the positive class over 96% of the time. We have more false positives than I\u0026rsquo;d like, but maybe some hyperparameter tuning can help here.\nStep 3: Finding Best Hyperparams with GridsearchCV For finding the best logistic regression hyperparams, we will use Scikit-Learn\u0026rsquo;s GridSearchCV in combination with an Imblearn Pipeline. GridsearchCV performs an exhaustive search over the provided parameters and performs cross-validation for every parameter combination. Imblearn\u0026rsquo;s Pipeline allows us to perform the same resampling technique we chose in step two with GridsearchCV. The pipeline will resample the training data using cluster undersampling but will not touch the balance of the test data in the fold. We\u0026rsquo;ll set up GridsearchCV to optimize for recall score and have it return the model that performed the best in terms of average recall across all folds.\nfrom imblearn.pipeline import Pipeline from sklearn.model_selection import GridSearchCV model = Pipeline([ # Perform resampling on train split (\u0026#39;sampling\u0026#39;, ClusterCentroids(estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=42)), (\u0026#39;classification\u0026#39;, LogisticRegression(random_state = 42)) ]) parameters = {\u0026#39;classification__solver\u0026#39;:(\u0026#39;liblinear\u0026#39;, \u0026#39;lbfgs\u0026#39;), \u0026#39;classification__C\u0026#39;:[1, 5, 10]} # Returns best model in terms of recall score best_grid_model = GridSearchCV(model, param_grid = parameters, cv = 5, refit = True, scoring = \u0026#39;recall\u0026#39;) best_grid_model.fit(X_train, y_train) # A look at how well all hyperparameter combinations perform pd.DataFrame(best_grid_model.cv_results_).sort_values(by = \u0026#39;rank_test_score\u0026#39;) mean_fit_time std_fit_time mean_score_time std_score_time param_classification__C param_classification__solver params split0_test_score split1_test_score split2_test_score split3_test_score split4_test_score mean_test_score std_test_score rank_test_score 2 29.715823 3.306074 0.068921 0.016334 5 liblinear {'classification__C': 5, 'classification__solv... 0.908257 0.935780 0.936364 0.909091 0.889908 0.915880 0.017857 1 3 29.644415 3.598677 0.095378 0.026857 5 lbfgs {'classification__C': 5, 'classification__solv... 0.908257 0.935780 0.936364 0.909091 0.889908 0.915880 0.017857 1 0 29.187842 2.767306 0.057547 0.019141 1 liblinear {'classification__C': 1, 'classification__solv... 0.899083 0.935780 0.927273 0.900000 0.889908 0.910409 0.017804 3 1 28.689645 3.572646 0.104958 0.027626 1 lbfgs {'classification__C': 1, 'classification__solv... 0.899083 0.935780 0.927273 0.900000 0.889908 0.910409 0.017804 3 4 28.605801 2.047266 0.052858 0.005150 10 liblinear {'classification__C': 10, 'classification__sol... 0.908257 0.926606 0.918182 0.890909 0.889908 0.906772 0.014572 5 5 32.235926 3.819624 0.099514 0.020922 10 lbfgs {'classification__C': 10, 'classification__sol... 0.908257 0.926606 0.918182 0.890909 0.889908 0.906772 0.014572 5 A look at the confusion matrix following hyperparameter tuning.\nshow_confusion(y_test, best_grid_model.predict(X_test)) This is a bit better. Our false positive rate has reduced slightly, meaning that this new model is having an easier time differentiating between the two classes than before. In many cases this would suffice, but what if we attempted to let no positive samples slip past our classifier? This is where the fourth step can come in, altering the decision threshold of the model.\nStep 4: Altering the Decision Threshold Altering the decision threshold will allow us to catch more true positive samples by increasing the sensitivity of the model. This increase in sensitivity will come at a cost, however, causing a decrease in the precision of our model. But, that\u0026rsquo;s ok! We are most concerned with catching the true positives in this data and aren\u0026rsquo;t nearly as worried about false positives. Below is a look at our model\u0026rsquo;s precision-recall curve to get a better understanding of the tradeoff between the two measures prior to changing the decision threshold.\nfrom sklearn.metrics import precision_recall_curve # Get class probability scores y_scores = best_grid_model.predict_proba(X_test)[:, 1] # Returns precision-recall pairs for different probability thresholds p, r, thresholds = precision_recall_curve(y_test, y_scores) viz_df = pd.DataFrame({\u0026#39;Precision\u0026#39;:p, \u0026#39;Recall\u0026#39;:r}) sns.lineplot(data = viz_df, x = \u0026#34;Recall\u0026#34;, y = \u0026#34;Precision\u0026#34;) For us to achieve perfect recall, the model\u0026rsquo;s precision is going to suffer quite a bit. Again, that\u0026rsquo;s OK because of this project\u0026rsquo;s main goal to effectively catch positive samples. Let\u0026rsquo;s iteratively lower the decision threshold until we achieve perfect recall.\ndef adjusted_classes(y_scores, t): # If pred probability is greater than or equal to threshold it is positive return [1 if y \u0026gt;= t else 0 for y in y_scores] # Default sklearn logistic regression threshold init_threshold = 0.5 pred = adjusted_classes(y_scores, init_threshold) recall = recall_score(y_test, pred) # Stops when recall is perfect while recall != 1.0: init_threshold = init_threshold - 0.001 pred = adjusted_classes(y_scores, init_threshold) recall = recall_score(y_test, pred) print(\u0026#34;Maximum Threshold for Perfect Recall:\u0026#34;, init_threshold) show_confusion(y_test, pred) Maximum Threshold for Perfect Recall: 0.3679999999999999 There you have it - a very sensitive model that doesn\u0026rsquo;t always spit out the positive class!\n","permalink":"https://chandleru11.github.io/posts/review_classifier/","summary":"\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n\u003cscript async src=\"https://www.googletagmanager.com/gtag/js?id=G-0NTZD30YVX\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-0NTZD30YVX');\n\u003c/script\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\n\u003ch2 id=\"class-imbalance-and-problem-statement\"\u003eClass Imbalance and Problem Statement\u003c/h2\u003e\n\u003cp\u003eClass imbalance is a common problem when building classifiers in the machine learning world, and our awesome previously-scraped \u003ca href=\"https://chandleru11.github.io/posts/review_scrape/\"\u003ecroc reviews data\u003c/a\u003e is unfortunately not so awesome from a class balance standpoint. Soon, we\u0026rsquo;ll assign binary class labels based on the rating a customer gave with their review where we\u0026rsquo;ll consider ratings of 2 stars (out of 5) or less to be negative sentiment and the remaining reviews as positive sentiment. As you\u0026rsquo;ll see in a moment, the vast majority of reviews belong to the positive sentiment class, and I think that\u0026rsquo;s great!\u003c/p\u003e","title":"Imbalanced Product Review Sentiment Classification"},{"content":"\r\u003c!DOCTYPE html\u003e About This post contains the dashboard I built to go along with my data cleaning project where I clean up my own running data from college. Shoutout to Andy Kriebel and his awesome YouTube video for getting me started!\nThe dashboard allows for dynamic exploration of the time series data from years down to days. I would recommend opening up the visualization to full screen, so you can see the \u0026ldquo;timeframe snapshots\u0026rdquo; along with my most important running stats.\nThe Dashboard \u003c!DOCTYPE html\u003e ","permalink":"https://chandleru11.github.io/posts/tableau_viz/","summary":"\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n\u003cscript async src=\"https://www.googletagmanager.com/gtag/js?id=G-0NTZD30YVX\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-0NTZD30YVX');\n\u003c/script\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\n\u003ch1 id=\"about\"\u003eAbout\u003c/h1\u003e\n\u003cp\u003eThis post contains the dashboard I built to go along with my \u003ca href=\"https://chandleru11.github.io/posts/clean_data/\"\u003edata cleaning project\u003c/a\u003e where I clean up my own running data from college. Shoutout to Andy Kriebel and his awesome YouTube \u003ca href=\"https://www.youtube.com/watch?v=EZMLjMaZYSs\u0026t=308s\"\u003evideo\u003c/a\u003e for getting me started!\u003c/p\u003e\n\u003cp\u003eThe dashboard allows for dynamic exploration of the time series data from years down to days. I would recommend opening up the visualization to full screen, so you can see the \u0026ldquo;timeframe snapshots\u0026rdquo; along with my most important running stats.\u003c/p\u003e","title":"Running Data Dashboard"},{"content":"\r\u003c!DOCTYPE html\u003e Motivation In my last post where I scraped reviews for Crocs Clogs, I mentioned that I often find myself wishing for a succinct summary of the reviews for a product. Let\u0026rsquo;s flesh that out a bit more. What I mean when I say \u0026ldquo;succinct summary\u0026rdquo; is that I want a quick understanding of a specific aspect for a given product. For example, I know that crocs come in amazing colors already. I can see that in the photos. But, how do they fit? What about their comfort? I find myself often most concerned with a specific aspect of a product such as those. I want to know what people are typically saying about fit and comfort. Many retailers offer a search bar for reviews, so you can filter reviews on a keyword. BUT, searching for \u0026ldquo;fit\u0026rdquo; across all crocs reviews would return a ton of samples, and how can we know which ones are representative of the general sentiment people have in regards to fit? What if we could give consumers a snapshot of the reviews containing a word or phrase they search for? Could we show them a small set of reviews that best represent all the reviews that mention the word \u0026ldquo;fit\u0026rdquo;, for example? I think we can!\nHow to Make it Happen Did someone say centroid-based clustering??? Because they would be correct. For this project we will be using the most popular centroid-based clustering algorithm, k-means.\nAccording to Wikipedia - k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.\nIn other words, k-means searches for the best representation of each cluster (the center) and assigns samples to a cluster based on their distance from each cluster center. So, we can say that a cluster\u0026rsquo;s center is an approximation of all the cluster\u0026rsquo;s members. Following this line of thinking, if we fit a k-means model to some data and only ask it to find 1 cluster, the calculated cluster center will act as a prototype for all the data that was passed to the model. To find the most representative subset of reviews for a particular keyword, we can filter the reviews based on the keyword, find a cluster center for the remaining samples, and get the X closest samples to the center. The closest samples to the center will be the most representative of the population. If this isn\u0026rsquo;t clear, check out the GIF below and imagine we\u0026rsquo;re only trying to make one cluster. Think about how the cluster center / centroid would move in that scenario.\nCredit - Sebastian Charmot, https://towardsdatascience.com/clear-and-visual-explanation-of-the-k-means-algorithm-applied-to-image-compression-b7fdc547e410\nTools Needed Of course we will need Pandas to make for easy data manipulation and usage. We also will need to clean up our text data, so we\u0026rsquo;ll use NLTK. Finally, we need a way to vectorize the text and fit a k-means model to it, so we\u0026rsquo;ll use Scikit-Learn for that part.\nimport pandas as pd from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.cluster import KMeans import nltk from nltk.corpus import stopwords from nltk.stem import PorterStemmer import re import string Clean Review Data It\u0026rsquo;s usually very important that we stem, remove punctuation, convert to lowercase, and remove stopwords from the text we\u0026rsquo;re fitting a model on. We don\u0026rsquo;t want \u0026ldquo;Oranges\u0026rdquo; and \u0026ldquo;orange\u0026rdquo; to be treated as different words, nor do we need words like \u0026ldquo;the\u0026rdquo; and \u0026ldquo;to\u0026rdquo;, for example.\ndef clean_text(text): # Remove punctuation text = re.sub(f\u0026#34;[{re.escape(string.punctuation)}]\u0026#34;, \u0026#34;\u0026#34;, text) # Convert to lowercase text = text.lower() # Remove stopwords nltk.download(\u0026#39;stopwords\u0026#39;, quiet=True) stop_words = set(stopwords.words(\u0026#39;english\u0026#39;)) tokens = nltk.word_tokenize(text) tokens = [word for word in tokens if word.lower() not in stop_words] # Stemming stemmer = PorterStemmer() tokens = [stemmer.stem(word) for word in tokens] # Join the tokens back into a single string cleaned_text = \u0026#39; \u0026#39;.join(tokens) return cleaned_text df = pd.read_csv(\u0026#39;data\\croc_reviews.csv\u0026#39;) df[\u0026#39;clean_review\u0026#39;] = [clean_text(text_to_clean) for text_to_clean in df[\u0026#39;review\u0026#39;]] df review date rating clean_review 0 !!!!!! E X C E L L E N T!!!!!!!!!! April 7, 2022 5.0 e x c e l l e n 1 \"They're crocs; people know what crocs are.\" April 3, 2021 5.0 theyr croc peopl know croc 2 - Quick delivery and the product arrived when ... March 19, 2023 5.0 quick deliveri product arriv compani said woul... 3 ...amazing \"new\" color!! who knew?? love - lov... July 17, 2022 5.0 amaz new color knew love love love 4 0 complaints from me; this is the 8th pair of ... June 4, 2021 5.0 0 complaint 8th pair croc ive bought like two ... ... ... ... ... ... 9233 I will definitely be buying again in many colo... August 25, 2021 4.0 definit buy mani color reason 45 materi feel t... 9234 I wish I would have bought crocs a long time ago. April 8, 2021 5.0 wish would bought croc long time ago 9235 wonderful. Gorgeous blue; prettier in person! April 27, 2022 5.0 wonder gorgeou blue prettier person 9236 Wonerful. Very comfy, and there are no blister... April 8, 2021 5.0 woner comfi blister feet unlik brand one 9237 Work from home - high arch need good support a... May 22, 2023 5.0 work home high arch need good support comfort ... 9238 rows Ã— 4 columns\nSome Notes on Vectorization Methods Since it\u0026rsquo;s been decided we\u0026rsquo;re using k-means for this project, we\u0026rsquo;re going to try out two vectorization methods and compare their performance. For each of our vectorization methods, we\u0026rsquo;ll also compute an example of their output using the two phrases \u0026ldquo;going to the store to buy bananas\u0026rdquo; and \u0026ldquo;we buy bananas\u0026rdquo; here. The vectorizers are the following:\nTF-IDF - term frequency-inverse document frequency helps by giving us an understanding of how important a word is within a document relative to an entire corpus. It is calculated by multiplying a word\u0026rsquo;s term frequency (TF) with its inverse document frequency (IDF). TF = # of times word appears in a document / total # of terms in document IDF = log(# of documents in corpus / # of documents in corpus that contain term) TF-IDF = Tf X IDF Sample bananas buy going store the to we \u0026ldquo;going to the store to buy bananas\u0026rdquo; 0.25 0.25 0.35 0.35 0.35 0.71 0.0 \u0026ldquo;we buy bananas\u0026rdquo; 0.50 0.50 0.0 0.0 0.0 0.0 0.70 CountVectorizer - Count vectorization converts a collection of text documents into a matrix of token counts. Every word in the corpus gets its own column, so every document is converted into a vector containing the frequency of its words. Sample bananas buy going store the to we \u0026ldquo;going to the store to buy bananas\u0026rdquo; 1 1 1 1 1 2 0 \u0026ldquo;we buy bananas\u0026rdquo; 1 1 0 0 0 0 1 Finding Typical Reviews The workflow for finding the most typical reviews will be:\nFilter reviews on a keyword and rating (either positive or negative) Vectorize the reviews with TF-IDF or CountVectorizer Fit a single k-means cluster Calculate the distance of each sample to the center of the cluster and sort def get_tfidf(text): vectorizer = TfidfVectorizer() # Fit and transform review data X = vectorizer.fit_transform(text) return X def get_countvect(text): vectorizer = CountVectorizer() # Fit and transform review data X = vectorizer.fit_transform(text) return X def kmeans_distance(df, vect): # Vectorize reviews using TF-IDF if vect == \u0026#39;tfidf\u0026#39;: X = get_tfidf(df[\u0026#39;clean_review\u0026#39;]) # Vectorize reviews using CountVectorizer elif vect == \u0026#39;count\u0026#39;: X = get_countvect(df[\u0026#39;clean_review\u0026#39;]) else: print(\u0026#34;Provide a vectorization method\u0026#34;) # Fit one cluster on the data. kmeans = KMeans(n_clusters=1, random_state=0, n_init=100).fit(X) # Compute the distance for each sample to the center of the cluster distances = kmeans.transform(X)**2 return distances def find_most_typical(df, word, rating, vect, asc = True): # Let\u0026#39;s consider anything 4 stars and up a positive review if rating == \u0026#39;positive\u0026#39;: df = df[df[\u0026#39;rating\u0026#39;] \u0026gt;= 4] else: df = df[df[\u0026#39;rating\u0026#39;] \u0026lt; 4] # Clean word so it matches root word in cleaned reviews filter_word = \u0026#39; \u0026#39; + clean_text(word) + \u0026#39; \u0026#39; #Filter using clean word df = df[df[\u0026#39;clean_review\u0026#39;].str.contains(filter_word, regex = True)] # Retrieve review distance to center of kmeans cluster df[\u0026#39;distance_to_center\u0026#39;] = kmeans_distance(df, vect) print(\u0026#34;There are\u0026#34;, len(df), rating, \u0026#34;reviews that mention:\u0026#34;, word) if asc == True: print(\u0026#34;The most typical reviews are:\u0026#34;) else: print(\u0026#34;The most atypical reviews are:\u0026#34;) for each in df.sort_values(by = \u0026#39;distance_to_center\u0026#39;, ascending = asc)[\u0026#39;review\u0026#39;].tolist()[:3]: print(\u0026#39;# \u0026#39;, each) Pseudo-evaluation We don\u0026rsquo;t really have a way to evaluate this model, so we\u0026rsquo;re going to have to use some intuition! Firstly, let\u0026rsquo;s look at what the 3 most typical positive reviews that mention \u0026ldquo;crocs\u0026rdquo; using TF-IDF and CountVectorizer would say.\nfind_most_typical(df, \u0026#39;crocs\u0026#39;, \u0026#39;positive\u0026#39;, \u0026#39;tfidf\u0026#39;) There are 2268 positive reviews that mention: crocs Most typical reviews are: # I love to wear these crocs because they are so comfortable. # I love wearing my crocs. They are so comfortable. # I love my pair of Crocs. They are so comfortable! find_most_typical(df, \u0026#39;crocs\u0026#39;, \u0026#39;positive\u0026#39;, \u0026#39;count\u0026#39;) There are 2268 positive reviews that mention: crocs Most typical reviews are: # I own a few pairs of crocs and love them all. # I love my Crocs; they are so comfortable. # I love Crocs; they are so comfortable! To make sure we\u0026rsquo;re on the right track, what do the most atypical positive reviews for crocs say? We can find the samples that are farthest from the center by sorting their distance greatest to smallest.\nfind_most_typical(df, \u0026#39;crocs\u0026#39;, \u0026#39;positive\u0026#39;, \u0026#39;tfidf\u0026#39;, asc = False) There are 2268 positive reviews that mention: crocs Most atypical reviews are: # Lids adore their Crocs! Fun with swapping charms and accessoryizing. # Purr Nickis Impact Crocs. Ima need you to give my girl a ha check!!!! # You can turn any none-croc person into a croc lover. find_most_typical(df, \u0026#39;crocs\u0026#39;, \u0026#39;positive\u0026#39;, \u0026#39;count\u0026#39;, asc = False) There are 2268 positive reviews that mention: crocs Most atypical reviews are: # Do you really need one more Croc review to convince you to buy these shoes? Never in a kabillion years did I think I'd ever buy a pair of Crocs. Nope, not my style, not my vibe, and not me, ever. But the husband had plantar fasciitis in one of his feet, and the pain was incredible. There are several options for shoes, but we're on a budget, and I kept reading about Crocs as a good starting point. So I ordered him some Crocs in the olive green. Then I thought, \u0026quot;My husband is a big baby. I should order myself a pair, wear them around for a day, and rave about them so he'll immediately start wearing his own pair of Crocs and start healing his feet.\u0026quot; The thing is, I was totally gobsmacked. At first the prickly nubs in the footbed were a bit distracting, but as I started walking around in them, they became less noticeable, and within the first twenty minutes or so, my feet started to feel energized and soothed. I went outside and swept the deck and did some chicken chores, and I was just in heaven. My legs felt supported and I had this incredible feeling of a whole body support system, starting with the acupressure from the footbed nubs and the lightweight shoe material and the fabulous cushioning. These shoes haven't had enough time to mold to my feet, and I'm already blissed out wearing them. I got mine in the olive green, too. They're earthy and surprisingly cute. My husband hasn't worn a clog in his entire life and walked off wearing them for the first time as if little elves were sitting inside his shoes and if he stepped down too hard, they'd get squished and died. A few minutes later, I saw him disappear down the hill at the back of our property. He can't walk when he gets home, his feet are in that much pain. # I was looking for something to wear at night while I was on the AIDS/Lifecycle. It's a 545 mile bicycle ride from San Francisco to Los Angeles where we raise money for services to HIV+ patients. The ride involves six nights of camping. It's a huge event and the evenings involve a lot of walking. We are allowed 70lbs of gear, but we have to haul our gear to our tent site every night, so traveling light is important. I wanted one pair of shoes for the week that I could put on after riding my bike all day and they would be comfortable enough to wear all week. I wanted one pair of shoes that wasn't going to get icky if the grass was wet at the campsite in the morning. I wanted one pair of shoes that kept my feet warm while I was walking around the campsite in the evening. These shoes totally fit the bill. After beating up my feet all day in cleats on the bike, they were a welcome respite. They allowed air to flow, but my feet never got cold at night. Dirt just wiped off. If I got some gravel in them, it kicked right back out. These shoes were so comfortable I found myself next to the gear trucks digging through my bag to find them and put them on. I know a lot of athletes use Crocs after their activity to kick around in, and these shoes are the shiznit! # These are my first Crocs, and lavender seems to run *almost* a size small. I would say the size 8 Crocs in lavender fits closer to 7 than size 8 street shoes. For reference, I wear size 7 in street shoes (Vans, Converse, Franco Sarto boots, Timberland Kinsley boots), size 38 European shoes (Veja), size 8 running shoes (Mizuno Wave Riders), size 8 Vasque Mesa Trek hiking boots. If you can, try them on. When I measured my feet and checked the Crocs size chart, it said I was a size 10, which would have been way too big. Size 8 lavender fit me roomy but comfortable, whether I'm barefoot or wearing thick, fluffy socks, with or without the Crocs shoe strap. They're super comfy! Those are definitely a bit weird! But, it looks like we\u0026rsquo;ve built something that works. Very cool.\nLooking at More Keywords and Ratings Let\u0026rsquo;s see what people have to say about the comfort and fit of crocs that is positive using both our vectorization methods.\nfind_most_typical(df, \u0026#39;comfort\u0026#39;, \u0026#39;positive\u0026#39;, \u0026#39;tfidf\u0026#39;) There are 1847 positive reviews that mention: comfort Most typical reviews are: # The crocs are very comfortable. I love them. # The crocs are comfortable, and I love them. # I love the color and comfort of these crocs! find_most_typical(df, \u0026#39;comfort\u0026#39;, \u0026#39;positive\u0026#39;, \u0026#39;count\u0026#39;) There are 1847 positive reviews that mention: comfort Most typical reviews are: # The crocs are very comfortable. I love them. # The crocs are comfortable, and I love them. # I love them; they are so comfortable to wear. find_most_typical(df, \u0026#39;fit\u0026#39;, \u0026#39;positive\u0026#39;, \u0026#39;tfidf\u0026#39;) There are 834 positive reviews that mention: fit Most typical reviews are: # Love the color and fit just as comfortably as my other crocs. # I love these. They are a perfect fit and very comfortable. I love the color as well. # Very comfortable and great fit. I love them. find_most_typical(df, \u0026#39;fit\u0026#39;, \u0026#39;positive\u0026#39;, \u0026#39;count\u0026#39;) There are 834 positive reviews that mention: fit Most typical reviews are: # I love them, and they fit great. # They are a great fit and are comfortable. # These are so comfortable, and it fits perfect! What do they say about those aspects that is negative?\nfind_most_typical(df, \u0026#39;comfort\u0026#39;, \u0026#39;negative\u0026#39;, \u0026#39;tfidf\u0026#39;) There are 68 negative reviews that mention: comfort Most typical reviews are: # I loved Crocs until my shoe size went to 8.5. Now the 9 is too big \u0026amp; the 8 is too small. I can't really comfortably wear either pair. # I love crocs; they're my favorite. However, for some reason, their sizing has gotten inaccurate. I have an old pair of size 8 women's \u0026amp; it's a perfect comfortable fit, and this time I ordered the same size 8 women's, and it's a little too snug for my comfort, and my toes have less room than usual. # I love how comfortable this shoe is. However, I wish they had half sizes because I am a 10 1/2 in women and I knew the regular 10 was going to be small, so I got an 11. I didn't like how big it made my feet look. I hope they change their sizing and add half sizes too. find_most_typical(df, \u0026#39;comfort\u0026#39;, \u0026#39;negative\u0026#39;, \u0026#39;count\u0026#39;) There are 68 negative reviews that mention: comfort Most typical reviews are: # I like how comfortable they are, but they are not true to size because they are too small. # The sole isn't comfortable for my feet. Standing long in the pair can be painful. # I bought these for my daughter. She said these are not comfortable like true clogs. find_most_typical(df, \u0026#39;fit\u0026#39;, \u0026#39;negative\u0026#39;, \u0026#39;tfidf\u0026#39;) There are 169 negative reviews that mention: fit Most typical reviews are: # I have over 20 pairs of crocs, and lately, the last 5 pairs I've purchased have all fit differently. I'm usually a men's 8 \u0026amp; recently bought a red pair of the classic clog. They were entirely too big, which made no sense because I have pink ones the same size that fit perfectly. I purchased a purple pair \u0026amp; decided to size down \u0026amp; get a men's 7 \u0026amp; they were way too small (which also made no sense because I have blue crocs the same size that were a more snug fit). My suggestion is to either make half sizes, or stop with this whole \u0026quot;Roomy Fit\u0026quot; thing that you all are doing. There is zero reason why each pair of crocs should have a different fit. I'll never order crocs online again. I highly recommend just going to the store to make your purchase. The return process is also very strenuous because Crocs does not offer exchanges. So now I have to send them back to the store via UPS, wait for my return to be processed, then wait until I can make my way to a crocs store because the closest store in my area is 36.6 miles away. Ridiculous. # Well, the crocs do not fit my granddaughter. One is actually a different size than the other. She received another pair of Crocs from her dad, and even though the pair he got her are size 9, and the pair I got her are size 9, the pair he bought her fits and the ones I got her do not. One pair was made in China and one pair was made in Vietnam. # I have had several pairs of crocs in the past, different styles and colors. Direct from the company. I have been disappointed in the consistency of the sizing. I had a size 10 in the classic style and wanted another pair in a different color. When they arrived, the fit was at least a size larger and wider than my original pair. Then I exchanged for a size 9. This fit better, but now the left shoe is smaller than the right. Not happy. find_most_typical(df, \u0026#39;fit\u0026#39;, \u0026#39;negative\u0026#39;, \u0026#39;count\u0026#39;) There are 169 negative reviews that mention: fit Most typical reviews are: # I ordered size 11 because they r too big. I thought they fit to size. # This pair fits a little short. # One pair of my crocs was a perfect fit, but the other fit was weird. Conclusion Well that was fun! After going through and reading the model outputs, it seems that CountVectorizer works the best for solving this problem. Which makes sense considering we are most concerned with finding the most \u0026ldquo;typical\u0026rdquo; reviews, and CountVectorizer focuses solely on term frequency to represent documents. Using CountVectorizer seems partial to the selection of shorter reviews, which I would consider an advantage in this space. Quick and succinct is the name of the game here. I really like this idea and may deploy a model based on this concept in the future\u0026hellip;\n","permalink":"https://chandleru11.github.io/posts/review_generizer/","summary":"\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n\u003cscript async src=\"https://www.googletagmanager.com/gtag/js?id=G-0NTZD30YVX\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-0NTZD30YVX');\n\u003c/script\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\n\u003ch1 id=\"motivation\"\u003eMotivation\u003c/h1\u003e\n\u003cp\u003eIn my \u003ca href=\"https://chandleru11.github.io/posts/review_scrape/\"\u003elast post\u003c/a\u003e where I scraped reviews for Crocs Clogs, I mentioned that I often find myself wishing for a succinct summary of the reviews for a product. Let\u0026rsquo;s flesh that out a bit more. What I mean when I say \u0026ldquo;succinct summary\u0026rdquo; is that I want a quick understanding of a specific aspect for a given product. For example, I know that crocs come in amazing colors already. I can see that in the photos. But, how do they fit? What about their comfort? I find myself often most concerned with a specific aspect of a product such as those. I want to know what people are typically saying about fit and comfort. Many retailers offer a search bar for reviews, so you can filter reviews on a keyword. BUT, searching for \u0026ldquo;fit\u0026rdquo; across all crocs reviews would return a ton of samples, and how can we know which ones are representative of the general sentiment people have in regards to fit? What if we could give consumers a snapshot of the reviews containing a word or phrase they search for? Could we show them a small set of reviews that best represent all the reviews that mention the word \u0026ldquo;fit\u0026rdquo;, for example? I think we can!\u003c/p\u003e","title":"Using Unsupervised ML to 'Typicalize' Product Reviews"},{"content":"\r\u003c!DOCTYPE html\u003e Motivation Before we get into things, here\u0026rsquo;s a link to download the dataset if you would like.\nDo you ever find yourself pouring over a product\u0026rsquo;s reviews trying to decide if it\u0026rsquo;s right for you? I sure do. Many of these times I\u0026rsquo;ve wished there was a quick summary of the reviews I could read to speed up the decision process. I\u0026rsquo;ve yet to see any online retailers doing exactly what I\u0026rsquo;m looking for, so I\u0026rsquo;ve decided to make my own review summarizer. But, we need some data for training such a tool.\nFor this project I wanted to use data that was fun, interesting, and maybe even a little controversial\u0026hellip; After much thought and deliberation over what product reviews to train a summary generator on, I landed on Crocs. Yes, I\u0026rsquo;m talking about the clunky rubber footwear my grandmother wears while gardening that some fashionistas would consider an abomination. What\u0026rsquo;s more fun than that??? Maybe or maybe not to your surprise, I searched the internet for a dataset containing reviews for Crocs to no avail. So, we\u0026rsquo;re going to have to find our own data. Follow along as I build a webscraper to farm reviews for Crocs Classic Clog, so we can train a review summarizer for the most loved and hated shoes on the planet!\nWhat tools can we use? There are several Python libraries that are helpful for webscraping:\nRequests: To send HTTP requests to websites. Beautiful Soup: For parsing HTML and XML documents. Selenium: For scraping dynamic web pages using browser automation. Pandas: To manipulate and store scraped data. In this project, I\u0026rsquo;ll be using Requests, Beautiful Soup, and Pandas. Selenium isn\u0026rsquo;t needed because we don\u0026rsquo;t need to click anything to move through the review pages on Zappos.com as you\u0026rsquo;ll see later.\nGetting Started Before we start scraping, we need to do some research, meaning that we need to go look around on the internet and assess potential sites for scraping. What sites would have a lot of product reviews for Crocs? The Crocs website itself, Amazon, and Zappos immediately come to mind. After following some initial site evaluation steps outlined here, I decided Zappos.com was the best place to scrape from.\nFirst, we need to figure out how to traverse the pages of reviews on Zappos.com. I went and clicked through the review pages and paid close attention to the URL. I noticed that it explicitly changes with each page. For example, https://www.zappos.com/product/review/7153812/page/1/orderBy/best takes you to the first page, and https://www.zappos.com/product/review/7153812/page/2/orderBy/best takes you to the second page. Easy. To traverse the pages, all we need to do is change a single character in the URL.\nSome Code to Download Whole Webpages I prefer to download whole web pages and extract data from them later. Why? Because companies oftentimes aren\u0026rsquo;t super happy about you scraping their data, and you have to be careful about how your traffic looks as you surf pages. Manners are important! Extracting data from HTML is a messy process that requires development, and you don\u0026rsquo;t want to risk having to rerun your scraper because you forgot to write the Beautiful Soup code to extract a product\u0026rsquo;s rating, for example. A script to traverse and download Zappos review pages is below.\nfrom bs4 import BeautifulSoup import requests import random import time HEADERS = ({\u0026#39;User-Agent\u0026#39;:\u0026#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.71 Safari/537.36\u0026#39;, \u0026#39;From\u0026#39;: \u0026#39;chandler_underwood@yahoo.com\u0026#39;}) #Download raw HTML def getdata(url): r = requests.get(url, headers=HEADERS, timeout=5) return r.text #Convert HTML to parsable string def html_code(url): htmldata = getdata(url) soup = BeautifulSoup(htmldata, \u0026#39;html.parser\u0026#39;) return (soup) text = \u0026#34;https://www.zappos.com/product/review/7153812/page/{insert}/orderBy/best\u0026#34; #Iterate over all possible URLs for i in range(1, 1000): #Retrieve HTML print(\u0026#34;Scraping: \u0026#34; + text.format(insert = str(i))) code = html_code(text.format(insert = str(i))) #Save file with unique name file = open(\u0026#34;path_to_folder/Zappos_Croc_File_\u0026#34; + str(i) + \u0026#34;.html\u0026#34;,\u0026#34;w\u0026#34;) file.write(str(code)) file.close() #Always remember to be polite time.sleep(random.randint(15,40)) Extracting Data from Webpages Now that we\u0026rsquo;ve downloaded the pages of reviews, we need to figure out how to extract the text from the HTML in a useful format. Google Chrome\u0026rsquo;s page inspect tool is great here as it allows us to click on items within the webpage and find their respective HTML tag names. As I clicked on the items I wanted to extract on the webpage, I took note of the tag names associated with them. Beautiful Soup uses these tag names to extract the text we need. Through some trial and error, I ended up with the following function to extract the desired text to make the reviews dataset.\ndef cus_rev_zappos(soup, b = True): review_text = [] date = [] rating = [] rating_type = [] rating_return = [] #Extract review for item in soup.find_all(\u0026#34;div\u0026#34;, class_=\u0026#34;Xl-z Yl-z\u0026#34;): review_text.append(item.get_text()) #Extract review date for item in soup.find_all(\u0026#34;time\u0026#34;): date.append(item.get_text()) #Extract product rating for item in soup.find_all(\u0026#34;span\u0026#34;, class_=\u0026#34;HQ-z\u0026#34;): rating.append(item.get_text()) #Extract product rating type (Overall, Style, Fit, etc.) for item in soup.find_all(\u0026#34;em\u0026#34;, class_=\u0026#34;Ql-z\u0026#34;): rating_type.append(item.get_text()) #We only care about overall product ratings for this project rating = list(zip(rating[5:], rating_type)) for x in rating: if x[1] == \u0026#39;Overall\u0026#39;: rating_return.append(x[0]) return review, date, rating_return I then iterated over and extracted the text from the downloaded web pages using the following code, and we got a decent amount of data! There are over 9,000 distinct reviews here. Below is a glance at the data.\nfrom tqdm.notebook import tqdm reviews, ratings, dates = [], [], [] for x in tqdm(range(len(filenames))): f = open(\u0026#34;path_to_folder/\u0026#34; + filenames[x], \u0026#39;r\u0026#39;) soup = BeautifulSoup(f.read(), \u0026#39;html.parser\u0026#39;) review, date, rating = cus_rev_zappos(soup) reviews.extend(review) ratings.extend(rating) dates.extend(date) review_df = pd.DataFrame({\u0026#39;review\u0026#39;: reviews, \u0026#39;date\u0026#39;: date, \u0026#39;rating\u0026#39;: ratings}) review_df.to_csv(\u0026#34;Zappos_Croc_Reviews_Total.csv\u0026#34;) review_df.head(10) review date rating 0 !!!!!! E X C E L L E N T!!!!!!!!!! April 7, 2022 5.0 1 \"They're crocs; people know what crocs are.\" April 3, 2021 5.0 2 - Quick delivery and the product arrived when ... March 19, 2023 5.0 3 ...amazing \"new\" color!! who knew?? love - lov... July 17, 2022 5.0 4 0 complaints from me; this is the 8th pair of ... June 4, 2021 5.0 5 10/10 I ran from the cops in mine, and they ar... July 2, 2023 5.0 6 The 11 year old granddaughter loves them!!!!!! August 7, 2022 5.0 7 The 11 yr old daughter loves them. August 7, 2022 5.0 8 12M classic clogs made in Vietnam are great, a... April 25, 2023 1.0 9 My 13 year old grandson chose this color and l... March 13, 2023 5.0 Let\u0026rsquo;s make a wordcloud to get some understanding of the most popular words in the dataset.\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator from os import path from PIL import Image import matplotlib.pyplot as plt # Start with one review: text = \u0026#39; \u0026#39;.join(review_df[\u0026#39;review\u0026#39;].to_list()).lower() # Create and generate a word cloud image: wordcloud = WordCloud(max_words=50, background_color=\u0026#34;white\u0026#34;).generate(text) # Display the generated image: plt.imshow(wordcloud, interpolation=\u0026#39;bilinear\u0026#39;) plt.axis(\u0026#34;off\u0026#34;) plt.show() \u0026ldquo;Croc\u0026rdquo; and \u0026ldquo;Love\u0026rdquo; seem to go together a lot. Why am I not surprised? :) I\u0026rsquo;m excited to see what I can do with this data in the future! Again, here\u0026rsquo;s the link to download the dataset if you\u0026rsquo;d like.\n","permalink":"https://chandleru11.github.io/posts/review_scrape/","summary":"\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n\u003cscript async src=\"https://www.googletagmanager.com/gtag/js?id=G-0NTZD30YVX\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-0NTZD30YVX');\n\u003c/script\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\n\u003ch1 id=\"motivation\"\u003eMotivation\u003c/h1\u003e\n\u003cp\u003eBefore we get into things, here\u0026rsquo;s a \u003ca href=\"https://www.kaggle.com/datasets/chandlerunderwood/crocs-clog-reviews/data\"\u003elink\u003c/a\u003e to download the dataset if you would like.\u003c/p\u003e\n\u003cp\u003eDo you ever find yourself pouring over a product\u0026rsquo;s reviews trying to decide if it\u0026rsquo;s right for you? I sure do. Many of these times I\u0026rsquo;ve wished there was a quick summary of the reviews I could read to speed up the decision process. I\u0026rsquo;ve yet to see any online retailers doing exactly what I\u0026rsquo;m looking for, so I\u0026rsquo;ve decided to make my own review summarizer. But, we need some data for training such a tool.\u003c/p\u003e","title":"Building a Product Reviews Webscraper"},{"content":"\r\u003c!DOCTYPE html\u003e Introduction For those of you that don\u0026rsquo;t know me personally, I managed to run collegiately for 6 years (thanks Covid?). Over that time I logged a lot of miles and a lot of Garmin activities. I still run quite a bit, but my barn burning days are behind me. I\u0026rsquo;d like to build a dashboard to get some insights to my running trends during that time as a sort of \u0026ldquo;last hoorah\u0026rdquo;, but sadly, a lot of my running data is missing and messy. I think cleaning it up will make for a great project to test my skills! Follow along here as I clean up and fill-in my running data using various techniques such as pulling outside data sources and training some ML models to predict missing values.\nData Read-in and Initial Exploration Below is a first look at my running data from college. Over the span of 6 years, I went for a run at least 1,975 times!\nimport pandas as pd data_path = \u0026#34;Activities 20\u0026#34; df = pd.read_csv(data_path + \u0026#34;17.csv\u0026#34;) for i in range (18,23): df = pd.concat([df, pd.read_csv(data_path + str(i) + \u0026#34;.csv\u0026#34;)]) df = df.reset_index().drop(columns = \u0026#39;index\u0026#39;) df Activity Type Date Favorite Title Distance Calories Time Avg HR Max HR Avg Run Cadence ... Min Temp Surface Interval Decompression Best Lap Time Number of Laps Max Temp Moving Time Elapsed Time Min Elevation Max Elevation 0 Running 2017-12-31 10:48:59 False Sevierville Running 13.62 1,462 01:29:31 0 0 175 ... 0.0 0:00 No 00:00.00 14 0.0 01:29:30 01:32:18 958 1,181 1 Running 2017-12-30 08:44:33 False Sevierville Running 5.83 631 00:41:23 0 0 176 ... 0.0 0:00 No 00:00.00 6 0.0 00:41:25 00:41:35 1,001 1,174 2 Running 2017-12-29 11:21:54 False Sevierville Running 8.22 881 00:50:45 0 0 176 ... 0.0 0:00 No 00:00.00 10 0.0 00:50:45 00:51:27 968 1,167 3 Running 2017-12-29 11:06:10 False Sevierville Running 1.97 209 00:13:42 0 0 174 ... 0.0 0:00 No 00:00.00 2 0.0 00:13:41 00:14:01 1,028 1,178 4 Running 2017-12-28 06:24:02 False Moss Point Running 7.37 797 00:52:04 0 0 173 ... 0.0 0:00 No 00:00.00 8 0.0 00:52:03 00:52:16 64 135 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1970 Running 2022-01-05 08:38:41 False Starkville Running 11.50 1,014 01:19:58 147 163 178 ... 57.2 0:00 No 01:19:57.54 1 75.2 01:19:48 01:22:26 223 402 1971 Running 2022-01-04 10:30:42 False Noxubee County Running 11.01 851 01:07:48 145 169 183 ... 51.8 0:00 No 02:01.19 14 69.8 01:07:39 01:23:25 130 201 1972 Running 2022-01-03 10:29:42 False Starkville Running 15.01 1,349 01:43:07 148 171 179 ... 37.4 0:00 No 01:43:06.81 1 77.0 01:43:04 01:51:40 89 243 1973 Running 2022-01-02 09:07:55 False Leon County Running 8.01 688 00:56:25 141 156 177 ... 77.0 0:00 No 56:24.51 1 86.0 00:56:22 00:58:52 -94 167 1974 Running 2022-01-01 09:45:26 False Tallahassee Running 7.00 593 00:51:11 136 159 175 ... 75.2 0:00 No 51:10.60 1 84.2 00:51:06 00:56:12 -59 205 1975 rows Ã— 38 columns\nLet\u0026rsquo;s have a look at the columns and datatypes of our dataset.\ndf.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; RangeIndex: 1975 entries, 0 to 1974 Data columns (total 38 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Activity Type 1975 non-null object 1 Date 1975 non-null object 2 Favorite 1975 non-null bool 3 Title 1975 non-null object 4 Distance 1975 non-null float64 5 Calories 1975 non-null object 6 Time 1975 non-null object 7 Avg HR 1975 non-null int64 8 Max HR 1975 non-null int64 9 Avg Run Cadence 1975 non-null object 10 Max Run Cadence 1975 non-null object 11 Avg Pace 1975 non-null object 12 Best Pace 1975 non-null object 13 Total Ascent 1975 non-null object 14 Total Descent 1975 non-null object 15 Avg Stride Length 1975 non-null float64 16 Avg Vertical Ratio 1975 non-null float64 17 Avg Vertical Oscillation 1975 non-null float64 18 Avg Ground Contact Time 1975 non-null int64 19 Training Stress ScoreÂ® 1975 non-null float64 20 Avg Power 1975 non-null int64 21 Max Power 1975 non-null int64 22 Grit 1975 non-null float64 23 Flow 1975 non-null float64 24 Avg. Swolf 1975 non-null int64 25 Avg Stroke Rate 1975 non-null int64 26 Total Reps 1975 non-null int64 27 Dive Time 1975 non-null object 28 Min Temp 1975 non-null float64 29 Surface Interval 1975 non-null object 30 Decompression 1975 non-null object 31 Best Lap Time 1975 non-null object 32 Number of Laps 1975 non-null object 33 Max Temp 1975 non-null float64 34 Moving Time 1975 non-null object 35 Elapsed Time 1975 non-null object 36 Min Elevation 1975 non-null object 37 Max Elevation 1975 non-null object dtypes: bool(1), float64(9), int64(8), object(20) memory usage: 573.0+ KB As you may have guessed, after reading Garmin\u0026rsquo;s documentation, many of the data\u0026rsquo;s attributes are not useful to us as they are not metrics taken for runs such as Max Power (a cycling metric) and Avg Stroke Rate (a swimming metric). In the cleanup and feature engineering section, we\u0026rsquo;ll drop those and many others that aren\u0026rsquo;t helpful for understanding my running performances.\nThe first issue with this dataset is that the Avg HR and Max HR columns are populated with some zeros (see table above), and I assure you that my heart was beating faster than that! The Max Temp and Min Temp columns also contain some zeroes. This is because I didn\u0026rsquo;t have a fancy watch in the beginning of college that logged those metrics. Because of this we can assume that the 0\u0026rsquo;s populating those four columns are actually NULL / missing values. Many of our columns should contain numerical data but instead contain strings such as Min Elevation, so we are going to fix that in the next section too.\nData Cleaning and Feature Engineering First thing we need to do is drop those extraneous columns and correct the data types for our remaining columns.\nimport numpy as np #Drop extraneous columns cols_to_keep = [\u0026#39;Date\u0026#39;, \u0026#39;Title\u0026#39;, \u0026#39;Time\u0026#39;, \u0026#39;Avg Pace\u0026#39;, \u0026#39;Best Pace\u0026#39;, \u0026#39;Distance\u0026#39;, \u0026#39;Calories\u0026#39;, \u0026#39;Avg HR\u0026#39;, \u0026#39;Max HR\u0026#39;, \u0026#39;Avg Run Cadence\u0026#39;, \u0026#39;Max Run Cadence\u0026#39;, \u0026#39;Total Ascent\u0026#39;, \u0026#39;Total Descent\u0026#39;, \u0026#39;Avg Stride Length\u0026#39;, \u0026#39;Min Temp\u0026#39;, \u0026#39;Max Temp\u0026#39;, \u0026#39;Min Elevation\u0026#39;, \u0026#39;Max Elevation\u0026#39;] df = df[cols_to_keep] #Replace missing values with NaN for easy pandas manipulation df = df.replace(\u0026#39;--\u0026#39;, np.nan) #String Garmin uses in place of NaN df = df.replace(0.0, np.nan) df = df.replace(0, np.nan) #Remove commas so we can convert these columns to numerical data cols_to_clean = [\u0026#39;Calories\u0026#39;, \u0026#39;Total Ascent\u0026#39;, \u0026#39;Total Descent\u0026#39;, \u0026#39;Min Elevation\u0026#39;, \u0026#39;Max Elevation\u0026#39;] df[cols_to_clean] = df[cols_to_clean].replace({\u0026#39;,\u0026#39;:\u0026#39;\u0026#39;}, regex=True) #Conversion of columns to floats for use in models def float_convert(col): df[col] = df[col].astype(float) float_convert(cols_to_keep[5:]) There are a few important columns that are written in a time format that is useful for humans but not machines. Let\u0026rsquo;s engineer some new features using them.\n#Drop activities saved by accident df = df[df[\u0026#39;Avg Pace\u0026#39;].notna()] df = df[df[\u0026#39;Best Pace\u0026#39;].notna()] #Convert values to float representing an equal amount of time in minutes df[\u0026#39;Total Run Time\u0026#39;] = [60 * float(x.split(\u0026#39;:\u0026#39;)[0]) + float(x.split(\u0026#39;:\u0026#39;)[1]) + (float(x.split(\u0026#39;:\u0026#39;)[2].split(\u0026#39;.\u0026#39;)[0])/60) for x in df[\u0026#39;Time\u0026#39;]] df.drop(columns = \u0026#39;Time\u0026#39;, inplace = True) df[\u0026#39;Avg Pace\u0026#39;] = [float(x.split(\u0026#39;:\u0026#39;)[0]) + float(x.split(\u0026#39;:\u0026#39;)[1]) / 60 for x in df[\u0026#39;Avg Pace\u0026#39;]] df[\u0026#39;Best Pace\u0026#39;] = [float(x.split(\u0026#39;:\u0026#39;)[0]) + float(x.split(\u0026#39;:\u0026#39;)[1]) / 60 for x in df[\u0026#39;Best Pace\u0026#39;]] #My college running days ended on the date below df[\u0026#39;Date\u0026#39;] = pd.to_datetime(pd.to_datetime(df[\u0026#39;Date\u0026#39;]).dt.strftime(\u0026#39;%Y-%m-%d\u0026#39;)) df = df[df[\u0026#39;Date\u0026#39;] \u0026lt; np.datetime64(\u0026#34;2022-05-15\u0026#34;)] df.info() \u0026lt;class 'pandas.core.frame.DataFrame'\u0026gt; Int64Index: 1883 entries, 0 to 1974 Data columns (total 18 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Date 1883 non-null datetime64[ns] 1 Title 1883 non-null object 2 Avg Pace 1883 non-null float64 3 Best Pace 1883 non-null float64 4 Distance 1883 non-null float64 5 Calories 1883 non-null float64 6 Avg HR 607 non-null float64 7 Max HR 607 non-null float64 8 Avg Run Cadence 1883 non-null float64 9 Max Run Cadence 1883 non-null float64 10 Total Ascent 1855 non-null float64 11 Total Descent 1861 non-null float64 12 Avg Stride Length 1883 non-null float64 13 Min Temp 607 non-null float64 14 Max Temp 607 non-null float64 15 Min Elevation 1875 non-null float64 16 Max Elevation 1880 non-null float64 17 Total Run Time 1883 non-null float64 dtypes: datetime64[ns](1), float64(16), object(1) memory usage: 279.5+ KB Over half of the values in the Avg HR, Max HR, Min Temp and Max Temp columns are NULL. Remember, I\u0026rsquo;m doing this, so I can get a better understanding of trends in my running data over the years I ran in college. I want to create some visualizations with this data in the future, and all these values are important in getting a \u0026ldquo;big picture\u0026rdquo; look at my running trends. To deal with the missing data, we have four options:\nDrop the rows that are missing data. Fill NULL rows with some sort of common value (oftentimes the median of the column in question). Bring in an outside data source. Create a predictive model. Option 1 is not going to work here as that would eliminate nearly two thirds of my data. Option 2 works OK for the columns that are missing only a few features, but it would definitely take away from the richness of the data and make for some boring / unhelpful visualizations if we used it for all the missing values in the dataset. But, option 3 can work great for filling in the temperature data as it is easy to find weather data, and option 4 is the way to go for fixing the HR data.\n#Using Option 2 to infill missing data with only a few NULLs cols_with_few_nan = [\u0026#39;Total Ascent\u0026#39;, \u0026#39;Total Descent\u0026#39;,\u0026#39;Min Elevation\u0026#39;, \u0026#39;Max Elevation\u0026#39;] df[cols_with_few_nan] = df[cols_with_few_nan].fillna(df[cols_with_few_nan].median()) Bringing in Some Outside Help Unfortunately, Garmin uses a somewhat cryptic system to log the location of runs. It usually titles each activity as either the county or city name plus \u0026ldquo;Running\u0026rdquo; with no other geolocation data to go along with it. To help us get started, let\u0026rsquo;s look at where my runs occurred that are missing temperature data.\nfrom collections import Counter run_locations = Counter(df[df[\u0026#39;Min Temp\u0026#39;].isna()][\u0026#39;Title\u0026#39;]) sorted(run_locations.items(), key=lambda x:x[1], reverse = True)[:10] [('Oktibbeha County Running', 349), ('Starkville Running', 245), ('Flowood Running', 127), ('Jackson County Running', 120), ('Moss Point Running', 64), ('Mobile County Running', 50), ('Boulder County Running', 43), ('Lucedale Running', 31), ('Oktibbeha County - Running', 21), ('Boulder Running', 21)] Despite differing titles, the vast majority of these samples occur either in my old college town of Starkville, MS or very close to it, and nearly all the rest occur somewhere in Mississippi or in the South. Because we don\u0026rsquo;t have a way to convert these titles to a more specific location without getting really messy, I believe it will suffice to use weather data for Starkville, MS as a proxy for all the missing values we have.\ndf_weather = pd.read_csv(\u0026#34;Weather Data.csv\u0026#34;) df_weather = df_weather[[\u0026#39;NAME\u0026#39;,\u0026#39;DATE\u0026#39;,\u0026#39;TMAX\u0026#39;,\u0026#39;TMIN\u0026#39;]].reset_index() df_weather.drop(columns = \u0026#39;index\u0026#39;, inplace = True) df_weather[\u0026#39;Date\u0026#39;] = pd.to_datetime(df_weather[\u0026#39;DATE\u0026#39;]) df_weather[\u0026#39;Min Temp\u0026#39;] = df_weather[\u0026#39;TMIN\u0026#39;] df_weather[\u0026#39;Max Temp\u0026#39;] = df_weather[\u0026#39;TMAX\u0026#39;] #Dataset contains weather reports from several locations surrounding Starkville, so we can group them together. df_weather = df_weather[[\u0026#39;Date\u0026#39;, \u0026#39;Min Temp\u0026#39;, \u0026#39;Max Temp\u0026#39;]].groupby(by = [\u0026#39;Date\u0026#39;]).mean() #Perform inner join, giving us a 1:1 ratio of dates to temperatures df = df.drop(columns = [\u0026#39;Min Temp\u0026#39;, \u0026#39;Max Temp\u0026#39;]).merge(df_weather, on = \u0026#39;Date\u0026#39;, how = \u0026#39;inner\u0026#39;) #Infill any remaining missing temperature values with the median cols_with_few_nan = [\u0026#39;Min Temp\u0026#39;, \u0026#39;Max Temp\u0026#39;] df[cols_with_few_nan] = df[cols_with_few_nan].fillna(df[cols_with_few_nan].median()) There you have it, our filled in temperature data. Now, we need to build a model(s) that can effectively populate the missing values in our Max HR and Avg HR columns.\nFitting a model to fill-in our missing data Let\u0026rsquo;s train and evaluate some regression models to fill in all that missing heart rate data. In the end we will have built two models, one to predict the Avg HR columns and another to predict Max HR.\n#Select subset of data with no missing values for training df_train = df.dropna() #Training features X_train = df_train[[\u0026#39;Avg Pace\u0026#39;, \u0026#39;Best Pace\u0026#39;, \u0026#39;Distance\u0026#39;, \u0026#39;Calories\u0026#39;, \u0026#39;Avg Run Cadence\u0026#39;, \u0026#39;Max Run Cadence\u0026#39;, \u0026#39;Total Ascent\u0026#39;, \u0026#39;Total Descent\u0026#39;, \u0026#39;Avg Stride Length\u0026#39;, \u0026#39;Min Elevation\u0026#39;, \u0026#39;Max Elevation\u0026#39;, \u0026#39;Total Run Time\u0026#39;, \u0026#39;Min Temp\u0026#39;, \u0026#39;Max Temp\u0026#39;]] y_avg = df_train[\u0026#39;Avg HR\u0026#39;] y_max = df_train[\u0026#39;Max HR\u0026#39;] Because of my running domain knowledge, I have an idea of what features will be useful for predicting the Max HR and Avg HR columns of our data, but I\u0026rsquo;m a fan of letting scikit-learn decide what features are best for me. Let\u0026rsquo;s select the best 5 features.\nfrom sklearn.feature_selection import SelectKBest, f_regression #The best features to predict Avg HR are not necessarily the best to predict Max HR kb_average = SelectKBest(f_regression, k=5).fit(X_train, y_avg) kb_max = SelectKBest(f_regression, k=5).fit(X_train, y_max) X_avg = kb_average.transform(X_train) X_max = kb_max.transform(X_train) Now we can use those extracted features to train several regression models and evaluate using cross validation to pick the best one for our two prediction tasks. The evaluation metrics we\u0026rsquo;ll use are Mean Absolute Error (MAE) and Mean Squared Error (MSE).\nfrom sklearn.model_selection import cross_validate from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression, Lasso from sklearn.svm import SVR from sklearn.metrics import make_scorer, mean_squared_error import statistics def cv(model, X, y, model_name): score = cross_validate(model, X, y, cv=5, scoring=(\u0026#39;neg_mean_absolute_error\u0026#39;, \u0026#39;neg_mean_squared_error\u0026#39;)) print(\u0026#34;\\nModel: \u0026#34;, model_name) print(\u0026#34;Test Mean Absolute Error: \u0026#34;, statistics.mean(score[\u0026#39;test_neg_mean_absolute_error\u0026#39;]), \u0026#34;\\nTest Mean Squared Error: \u0026#34;, statistics.mean(score[\u0026#39;test_neg_mean_squared_error\u0026#39;])) lasso = Lasso(alpha = 0.1) reg = LinearRegression() regr = SVR(C=1.0, epsilon=0.2) rfr = RandomForestRegressor(max_depth=10) data_list = [(X_avg, y_avg, \u0026#34;HR Avg\u0026#34;), (X_max, y_max, \u0026#34;HR Max\u0026#34;)] model_dict = {\u0026#34;Lasso Regression\u0026#34;:lasso, \u0026#34;Linear Regression\u0026#34;:reg, \u0026#34;SVR\u0026#34;:regr, \u0026#34;RF Regressor\u0026#34;:rfr} for data in data_list: print(\u0026#39;\\n############################\\n\u0026#39;,data[2]) for model in model_dict.keys(): cv(model_dict[model], data[0], data[1], model) ############################ HR Avg Model: Lasso Regression Test Mean Absolute Error: -3.4721680220384203 Test Mean Squared Error: -21.528693269323934 Model: Linear Regression Test Mean Absolute Error: -3.4965374931940674 Test Mean Squared Error: -21.794449648893643 Model: SVR Test Mean Absolute Error: -6.503686070502235 Test Mean Squared Error: -64.17778768337854 Model: RF Regressor Test Mean Absolute Error: -3.6666079473371216 Test Mean Squared Error: -22.542642038967205 ############################ HR Max Model: Lasso Regression Test Mean Absolute Error: -6.310069863144789 Test Mean Squared Error: -67.01035553775584 Model: Linear Regression Test Mean Absolute Error: -6.332889188711009 Test Mean Squared Error: -67.28914455377931 Model: SVR Test Mean Absolute Error: -7.571264852476413 Test Mean Squared Error: -91.18257365781352 Model: RF Regressor Test Mean Absolute Error: -5.748229756457544 Test Mean Squared Error: -57.09265923926437 Lasso regression is the best model for predicting the Average Heart Rate of my runs while Random Forest Regressor is the best at predicting the Max Heart Rate.\nFitting final models Let\u0026rsquo;s fit the best performing models using their entire respective training sets and predict on the samples that are missing HR data.\nlasso_avg = Lasso(alpha = 0.1) lasso_avg.fit(X_avg, y_avg) rfr_max = RandomForestRegressor(max_depth=10) rfr_max.fit(X_max, y_max) #Select predictive features from entire dataset X_full = df[[\u0026#39;Avg Pace\u0026#39;, \u0026#39;Best Pace\u0026#39;, \u0026#39;Distance\u0026#39;, \u0026#39;Calories\u0026#39;, \u0026#39;Avg Run Cadence\u0026#39;, \u0026#39;Max Run Cadence\u0026#39;, \u0026#39;Total Ascent\u0026#39;, \u0026#39;Total Descent\u0026#39;, \u0026#39;Avg Stride Length\u0026#39;, \u0026#39;Min Elevation\u0026#39;, \u0026#39;Max Elevation\u0026#39;, \u0026#39;Total Run Time\u0026#39;, \u0026#39;Min Temp\u0026#39;, \u0026#39;Max Temp\u0026#39;]].to_numpy() #Predict for all samples and infill rows that are missing values df[\u0026#39;Max HR\u0026#39;] = df[\u0026#39;Max HR\u0026#39;].combine_first(pd.Series(rfr_max.predict(kb_max.transform(X_full)).tolist())) df[\u0026#39;Avg HR\u0026#39;] = df[\u0026#39;Avg HR\u0026#39;].combine_first(pd.Series(lasso_avg.predict(kb_average.transform(X_full)).tolist())) df.to_csv(\u0026#39;Running_Data_Clean.csv\u0026#39;) df Date Title Avg Pace Best Pace Distance Calories Avg HR Max HR Avg Run Cadence Max Run Cadence Total Ascent Total Descent Avg Stride Length Min Elevation Max Elevation Total Run Time Min Temp Max Temp 0 2017-12-31 Sevierville Running 6.566667 5.983333 13.62 1462.0 179.348833 175.218101 175.0 187.0 381.0 425.0 1.40 958.0 1181.0 89.516667 22.0 46.0 1 2017-12-30 Sevierville Running 7.100000 6.533333 5.83 631.0 151.453714 168.844806 176.0 191.0 169.0 9.0 1.29 1001.0 1174.0 41.383333 27.0 49.0 2 2017-12-29 Sevierville Running 6.166667 5.350000 8.22 881.0 163.870123 176.137507 176.0 191.0 285.0 184.0 1.47 968.0 1167.0 50.750000 28.0 44.0 3 2017-12-29 Sevierville Running 6.950000 6.316667 1.97 209.0 139.974450 164.446402 174.0 191.0 48.0 181.0 1.34 1028.0 1178.0 13.700000 28.0 44.0 4 2017-12-28 Moss Point Running 7.066667 6.383333 7.37 797.0 156.137539 171.490540 173.0 185.0 182.0 195.0 1.32 64.0 135.0 52.066667 25.0 42.0 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1878 2022-01-05 Starkville Running 6.950000 6.316667 11.50 1014.0 147.000000 163.000000 178.0 201.0 420.0 404.0 1.30 223.0 402.0 79.966667 33.0 55.0 1879 2022-01-04 Noxubee County Running 6.166667 4.483333 11.01 851.0 145.000000 169.000000 183.0 232.0 289.0 246.0 1.42 130.0 201.0 67.800000 30.0 40.5 1880 2022-01-03 Starkville Running 6.866667 5.650000 15.01 1349.0 148.000000 171.000000 179.0 190.0 807.0 774.0 1.31 89.0 243.0 103.116667 29.5 51.0 1881 2022-01-02 Leon County Running 7.050000 5.716667 8.01 688.0 141.000000 156.000000 177.0 188.0 810.0 978.0 1.29 -94.0 167.0 56.416667 30.0 65.0 1882 2022-01-01 Tallahassee Running 7.316667 5.300000 7.00 593.0 136.000000 159.000000 175.0 186.0 801.0 863.0 1.26 -59.0 205.0 51.183333 61.5 79.0 1883 rows Ã— 18 columns\nThere you have it, a NULL-free, clean dataset. We are dashboard ready now. Check out my next post to see what I can make with this in Tableau.\n","permalink":"https://chandleru11.github.io/posts/clean_data/","summary":"\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n\u003cscript async src=\"https://www.googletagmanager.com/gtag/js?id=G-0NTZD30YVX\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-0NTZD30YVX');\n\u003c/script\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eFor those of you that don\u0026rsquo;t know me personally, I managed to run collegiately for 6 years (thanks Covid?). Over that time I logged a lot of miles and a lot of Garmin activities. I still run quite a bit, but my barn burning days are behind me. I\u0026rsquo;d like to build a dashboard to get some insights to my running trends during that time as a sort of \u0026ldquo;last hoorah\u0026rdquo;, but sadly, a lot of my running data is missing and messy. I think cleaning it up will make for a great project to test my skills! Follow along here as I clean up and fill-in my running data using various techniques such as pulling outside data sources and training some ML models to predict missing values.\u003c/p\u003e","title":"Cleaning Up Years of Daily Running Data"},{"content":"\r\u003c!DOCTYPE html\u003e Experience Blue Cross of Idaho Healthcare Data Analyst | June 2024 - Present\nDeveloped and optimized healthcare analytics reports to streamline reporting processes, saving hours of manual reporting time each week using Python, Pandas, and SQL Implemented and maintained hundreds of SQL queries to extract, manipulate, and analyze large datasets from relational databases to gain insights Collaborated with cross-functional teams to identify key performance indicators, creating custom data solutions to improve decision-making Boise State University Machine Learning Research Associate | September 2023 - January 2024\nMachine Learning Graduate Research Assistant | August 2022 â€“ August 2023\nDetected online news articles containing misinformation with 90% accuracy by implementing machine and deep learning models with Python, Scikit-Learn, and PyTorch Fine-tuned generative AIs using Transformers to identify and exploit vulnerabilities in otherwise effective online misinformation detectors, successfully reducing their detection accuracy by 50% Developed Natural Language Processing (NLP) pipelines using Pandas to preprocess complex misinformation datasets, significantly enhancing data quality for accurate model training Deployed online misinformation detectors to servers, achieving a processing capacity of over 500,000 predictions per hour for incoming data Charles Schwab Corporation Data Science and Analytics Intern | June 2020 â€“ August 2020\nDeveloped machine learning models using Python and Scikit-Learn to detect impending customer churn for one of Schwabâ€™s financial services, achieving a 78% true positive rate Led an 8-person analytics team in the collection and analysis of internship satisfaction data, resulting in actionable insights to improve intern retention strategies Leveraged Tableau visualizations to present the business impact of customer churn models along with intern analysis to cross-functional stakeholders Trustmark Bank Data Analytics Co-Op | June 2019 â€“ December 2019\nDeveloped an SQL and Python-based database API for a user interface to maintain unique identifiers during data warehouse migration process Transformed over 100 daily-run SQL queries to optimize performance and compatibility with data warehouse Created visualizations and daily reports to provide stakeholders with actionable insights Education Master of Science in Computer Science Boise State University | Graduation Date: August, 2023\nRelevant Coursework: Intro to Data Science Large-Scale Data Analysis Machine Learning Deep Learning Natural Language Processing Bachelor of Science in Computer Science, Mathematics Minor Mississippi State University | Graduation Date: May, 2021\nRelevant Coursework: Database Management Systems Data Structures and Algorithms Object-Oriented Programming Linear Algebra Statistical Inference and Probability Calculus I-IV ","permalink":"https://chandleru11.github.io/about/","summary":"\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n\u003cscript async src=\"https://www.googletagmanager.com/gtag/js?id=G-0NTZD30YVX\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-0NTZD30YVX');\n\u003c/script\u003e\n\u003c/head\u003e\n\u003c/html\u003e\n\n\u003chr\u003e\n\u003ch1 id=\"experience\"\u003eExperience\u003c/h1\u003e\n\u003ch2 id=\"blue-cross-of-idaho\"\u003eBlue Cross of Idaho\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eHealthcare Data Analyst\u003c/em\u003e | June 2024 - Present\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDeveloped and optimized healthcare analytics reports to streamline reporting processes, saving hours of manual reporting time each week using Python, Pandas, and SQL\u003c/li\u003e\n\u003cli\u003eImplemented and maintained hundreds of SQL queries to extract, manipulate, and analyze large datasets from relational databases to gain insights\u003c/li\u003e\n\u003cli\u003eCollaborated with cross-functional teams to identify key performance indicators, creating custom data solutions to improve decision-making\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"boise-state-university\"\u003eBoise State University\u003c/h2\u003e\n\u003cp\u003e\u003cem\u003eMachine Learning Research Associate\u003c/em\u003e | September 2023 - January 2024\u003c/p\u003e","title":"About"},{"content":"ReST Method for Fine-tuning LLMs using Huggingface Running Data Cleaner \u003c!DOCTYPE html\u003e ","permalink":"https://chandleru11.github.io/projects/","summary":"\u003ch1 id=\"rest-method-for-fine-tuning-llms-using-huggingface\"\u003eReST Method for Fine-tuning LLMs using Huggingface\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ChandlerU11/Hugging_ReST\"\u003e\u003cimg loading=\"lazy\" src=\"/sleepy_ducky.jpg\" alt=\"Alt Text\"  /\u003e\n\u003c/a\u003e\u003c/p\u003e\n\u003ch1 id=\"running-data-cleaner\"\u003eRunning Data Cleaner\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/ChandlerU11/Running_Data_Cleaner/tree/main\"\u003e\u003cimg loading=\"lazy\" src=\"/run_ducky.jpg\" alt=\"Alt Text\"  /\u003e\n\u003c/a\u003e\u003c/p\u003e\n\r\n\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n\u003chead\u003e\n\u003cscript async src=\"https://www.googletagmanager.com/gtag/js?id=G-0NTZD30YVX\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  window.dataLayer = window.dataLayer || [];\n  function gtag(){dataLayer.push(arguments);}\n  gtag('js', new Date());\n\n  gtag('config', 'G-0NTZD30YVX');\n\u003c/script\u003e\n\u003c/head\u003e\n\u003c/html\u003e","title":""},{"content":"Understanding Teenagersâ€™ Real and Fake News Sharing on Social Media Abstract: In this paper, we present results from our research work focused on understanding teenagersâ€™ real and fake news sharing on social media. Using existing real/fake news samples and best practices in qualitative, inductive data analysis, we identify factors that explain teenagersâ€™ sharing and not sharing behavior of real and fake news. Our findings suggest that influencing teenagersâ€™ decisions to share or not share news on social media requires changes in the knowledge of what is found in social media and enhancement of knowledge of the algorithmic effects of sharing or not sharing.\n\u003c!DOCTYPE html\u003e Link to Paper Adversarial Attacks on Fake News Detectors with Real User Comments Published; Link coming soon\n\u003c!DOCTYPE html\u003e ","permalink":"https://chandleru11.github.io/publications/","summary":"\u003ch1 id=\"understanding-teenagers-real-and-fake-news-sharing-on-social-media\"\u003eUnderstanding Teenagersâ€™ Real and Fake News Sharing on Social Media\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003eAbstract:\u003c/strong\u003e In this paper, we present results from our research work focused\non understanding teenagersâ€™ real and fake news sharing on social\nmedia. Using existing real/fake news samples and best practices\nin qualitative, inductive data analysis, we identify factors that explain teenagersâ€™ sharing and not sharing behavior of real and fake\nnews. Our findings suggest that influencing teenagersâ€™ decisions\nto share or not share news on social media requires changes in the\nknowledge of what is found in social media and enhancement of\nknowledge of the algorithmic effects of sharing or not sharing.\u003c/p\u003e","title":""}]